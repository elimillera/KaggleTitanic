---
title: "Machine Learning with the Caret package and the Titanic"
author: "Eli Miller"
date: "December 10, 2017"
output: 
  html_document:
    toc: true
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#The Caret Package

The caret package is a framework that unifies the methods used to create predictive models. Instead of function call for each model, every model is created by the same function that you pass a method to. The package also has visualization functions that are really good at analyzing complex relationships in data and are easier to implement then the base graphics package in R. Caret has tools that allow for data splitting on outcome, predictors, time series, and grouping variables. The caret package seeks to simplify the process for predictive analysis. It's [github page](http://topepo.github.io/caret/index.html) reads as a good introduction to data science.  
Beyond predictive uses, caret can be used to find results in probabilistic terms or determine variable importance. This is meant to showcase a few of the uses of the caret package. As with any technical subject, there is infinitely more that could be said for caret and machine learning in general.

```{r message = FALSE, warning = FALSE}
set.seed(1)
##Load packages for caret and parallel processing
library(caret)
library(parallel)
library(doParallel)

##Register cores
cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)
```
# Titanic Data
```{r}
raw.data <- read.csv("train.csv", na.strings = '')
```
## A Look at the Titanic Data
It's a good idea to look at the type of data you have, as well as to see how complete it is.
```{r}
str(raw.data)
```
Our training data has 12 features that are listed above.

```{r}
##See if any rows have missing data.
sapply(raw.data, anyNA)
##See if any rows are duplicates.
any(duplicated(raw.data))
```
The above showed us if there is any NA values in our data set, or if there are duplicated rows. Our age, cabin, and embarked variable have NA values. NA values cause the train function to fail, so we will have to do something with these features to make them usable. We have a choice to exclude these features or try to *impute* them, this would involve trying to predict the missing values. In a data set that has few predictors, imputation can help improve models but can also result in over fitting if used improperly. The model will predict on Class, Sex, Age(Imputed), SibSp, Parch, Fare, and Embarked(Imputed).  
I used the `dummyVars` function to change the Sex feature and the Embarked feature into numeric features rather then factors. As it stands, preprocessing in caret cannot impute factor features though this could change in the feuture.
I use the `preProcess` function to create an object that will predict the values with the k nearest neighbors function that is explained below. When we use the `predict` function, it returns the missing age values, as well as centers and scales the numeric columns.
```{r}
##Subset data for selected columns.
surv <- as.factor(raw.data$Survived)
selected.data <- subset(raw.data, select = c('Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked'))
##Create Dummy Variables to remove factor features.
dv <- dummyVars( ~., data = selected.data)
##Read dummy variables into new data frame.
dv.data <- predict(dv, newdata = selected.data)
##Make Preprocessed object.
pp <- preProcess(dv.data, method = 'knnImpute')
##Create object with preprocessed data
impute.data <- predict(pp, dv.data)
##Load imputed data into data frame.
imputed.data <- data.frame(dv.data)
imputed.data$Age <- impute.data[,4]
imputed.data$Survived <- raw.data$Survived
##Add embarked data
imputed.data[,8:10] <- impute.data[,8:10]
imputed.data$Survived <- surv
##Find correlation of features.
cor(imputed.data[1:7])
```
Using the correlation function on a data frame prints a matrix of correlation. We would want to drop variables that are very well correlated with each other, they don't tell us any new information and would only serve to complicate the model. Looking at the correlations, we don't see very many surprises, Survived is correlated with Fare, the more someone payed the more likely they were to survive, it's inversely correlated with Pclass, the higher the class, the less likely survival became. The Sex.female and Sex.male features are perfecly negitivly correlated so we can remove one and get the same effect.
```{r include = FALSE}
##Remove unneeded feature.
imputed.data <- imputed.data[,-imputed.data$Sex.male]
```

##Visualizaion the data with caret
The `featurePlot` function is used to create many different graphs that can illustrate groupings and behavior of data. 
```{r}
featurePlot(x = imputed.data[,2:7], y = as.factor(imputed.data$Survived), plot = 'pairs', auto.key = list(column = 2))
```
This particular plot shows how each pair of features interact. You can see a couple of clusters of blue which indicate survival. You can see clusters around higher fares,large number of family relationships, as well as higher age.

## A Machine Learning Model
I will be making a number of models and comparing their performance. This is one of the main benefits of using caret. You can use the same function, train(), for making all of the models. I am also using this package to partition the training data to create a set of data we can test on. This is done to see how much over-fitting is being done on our training set.
```{r}
train.model.ind <- createDataPartition(imputed.data$Survived, p = 0.8, list = FALSE)
train.model <- imputed.data[train.model.ind,]
test.model <- imputed.data[-train.model.ind,]
```

###Tuning Models with caret
Caret also allows you to precisely tune your models. The `trainControl` function can be passed arguments that control how the model is created. In my control method I am telling caret to use all my processors in parallel in the hopes that it will run faster. I also selected cross validation as my re sampling method. There's about a dozen different re sampling methods to select from that the `trainControl` method will accept. There is a trade off in resampling and efficiency, the increased performance of your model will diminish as you add more folds so having a large number of resamples will do little for your model and cause you to waste time waiting for your results.
```{r}
tc <- trainControl(allowParallel = TRUE, method = 'repeatedcv', number = 2, repeats = 2)
```


###Model Creation
```{r include = FALSE}
train.model$Survived <- as.factor(train.model$Survived)
test.model$Survived <- as.factor(test.model$Survived)
models.used <- c('kknn1', 'kknn2', 'rpart', 'bag','boost', 'nnet', 'svmLinear', 'rf')
```
There are [238 different models](https://topepo.github.io/caret/available-models.html) you can use in the caret package. I will be working on 11 of them. Not only are we interested in the accuracy , we are also interested about efficiency. I will be looking at the time it takes to process each of these models. This is not meant to be a resource on each of these methods, there is much more to know about all of these, but this should be a quick look on how accurate and efficient the methods are at their default settings.

####k Nearest Neighbors
```{r}
system.time({
    knnModel1 <- train(Survived ~ ., data = train.model, method = 'kknn', trainControl = tc)
    knnModel2 <- train(Survived ~ ., data = train.model, method = 'kknn', k = 3, l = 2, trainControl = tc)
})
```
This method finds the 'k' closest values in the training set and uses those to determine the classification, this is one of the simplest ways to classify. The first model defaults to k = 1. It just finds the closest value and uses that to determine our Survived variable. The second finds the closest three(k = 3), and uses the majority(l = 2) to indeterminate Survived.

####Classification Tree
```{r}
system.time({
    treeModel <- train(Survived ~ ., data = train.model, method = 'rpart')
})
```
CART(Classification and Regression Trees) models are sets of decisions that form a flowchart. These methods are popular because the final result can be easily interpreted by a data scientist, we will look at the tree created later. This readability can come at the expense of accuracy. 

####Bagging
```{r}
system.time({
    ##bagModel <- train(Survived ~ ., data = train.model, method = 'AdaBag', trainControl = tc)
})
```
Bagging involves creating a model out of other models, the AdaBag method we used creates a model built off classification trees. The models are averaged together, this reduces out-of-sample error.

####Boosting
```{r}
system.time({
    boostModel <- train(Survived ~ ., data = train.model, method = 'adaboost', trainControl = tc)
})
```
Boosting is related to bagging but instead of the models being averaged, more accurate models are given a higher weight, this can increase accuracy but increase out-of-sample error.

####Neural Network
```{r}
system.time({
    netModel <- train(Survived ~ ., data = train.model, method = 'nnet', trace = FALSE, trainControl = tc)
})
```
Neural networks are very handy for modeling relationships that are too complicated for linear models. The method we are using here is a single-hidden-layer network, the simplest type of neural network. Each of our variables is fed into a set of 'hidden layers' which each give outputs to the output layer. The idea is to find the weights of the inputs into the hidden layer and the weights from the hidden layer into the output layer. The goal is for the layers to form connections with each other, much like the neurons in the human brain.

####Support Vector Machine
```{r results = FALSE}
system.time({
    svmModel <- train(Survived ~ ., data = train.model, method = 'svmLinear', trainControl = tc)
})
```
SVMs(Support Vector Machines) map each observation to a n-space, where 'n' is the number of features of each observation, then try to find the vectors that result in the largest gap between classes. SVMs are robust, meaning they are not effected much by out-liars.

####Random Forest
```{r}

system.time({
    rfModel <- train(Survived ~ ., data = train.model, method = 'rf', trainControl = tc)
})
```
```{r}
stopCluster(cluster)
registerDoSEQ()
elapsedTime <- list( 9.91, 0.61, 684.11, 50.67, 2.72, 2.05, 6.95)
```
###Accuracy
Lets take a look at how well each prediction does with itself, and the test set we partitioned. We partitioned the data so we could see how much over fitting was resulting from our model choice. We can now estimate what the accuracy would be on a real data set where we didn't know the outcome. 
```{r include = FALSE}
knn1.mat <- confusionMatrix(train.model$Survived, predict(knnModel1, train.model))
knn2.mat <- confusionMatrix(train.model$Survived, predict(knnModel2, train.model))
rpart.mat <- confusionMatrix(train.model$Survived, predict(treeModel, train.model))
##bag.mat <- confusionMatrix(train.model$Survived, predict(bagModel, train.model)) Removed for easier computation.
boost.mat <- confusionMatrix(train.model$Survived, predict(boostModel, train.model))
net.mat <- confusionMatrix(train.model$Survived, predict(netModel, train.model))
svm.mat <- confusionMatrix(train.model$Survived, predict(svmModel, train.model))
rf.mat <- confusionMatrix(train.model$Survived, predict(rfModel, train.model))
train.accuracies <- c(knn1.mat$overall[1], knn2.mat$overall[1], rpart.mat$overall[1], 0.8137255, boost.mat$overall[1], net.mat$overall[1], svm.mat$overall[1], rf.mat$overall[1])

knn1.mat <- confusionMatrix(test.model$Survived, predict(knnModel1, test.model))
knn2.mat <- confusionMatrix(test.model$Survived, predict(knnModel2, test.model))
rpart.mat <- confusionMatrix(test.model$Survived, predict(treeModel, test.model))
##bag.mat <- confusionMatrix(test.model$Survived, predict(bagModel, test.model)) Removed for easier computation
boost.mat <- confusionMatrix(test.model$Survived, predict(boostModel, test.model))
net.mat <- confusionMatrix(test.model$Survived, predict(netModel, test.model))
svm.mat <- confusionMatrix(test.model$Survived, predict(svmModel, test.model))
rf.mat <- confusionMatrix(test.model$Survived, predict(rfModel, test.model))
test.accuracies <- c(knn1.mat$overall[1], knn2.mat$overall[1], rpart.mat$overall[1], 0.8305085, boost.mat$overall[1], net.mat$overall[1], svm.mat$overall[1], rf.mat$overall[1])
```
The accuracies were computed as the correct predictions divided by total observations.

```{r}
model.metas <- data.frame(row.names = models.used, train.accuracies, test.accuracies)
model.metas
plot(model.metas$test.accuracies, main = "Accuracy of the Models", xlab = "Models", ylab = "Accuracy", ylim = c(0.7, 1), xaxt = 'n', type = 'p', pch = 15, col = 'red')
points(model.metas$train.accuracies, col = 'blue', pch = 16)
axis(1, at = 1:8, labels = models.used)
```
Much of the variation we saw in our training sets is gone. We have a range of 20% for our training sets, but a range of around 9% for our testing set. This is because the choice of model doesn't matter all that much, our worst model does an okay job of predicting with 76%, and our best models isn't all that much better at 85%. Our top models on the training set, a k-nearest-neighbors and a boosted model, fell 21% and 16% suggesting a large amount of over fitting. What's also interesting is the models that have very little problem with overfitting. The rpart model, the bagged model, the neural network, and the svm model, all had small differences between the training and test set.
It looks like our top model is a random forest with a 84.7% accuracy rate on the test data. I will predict using the neural network as it also had one of the highest rates is testing yet had little out-of-sample error.
```{r}
##Read in test data with unknown results.
sub.data <- read.csv('test.csv')
ids <- sub.data$PassengerId
##Subset data for needed features.

##Subset data for selected columns.
sub.data <- subset(sub.data, select = c('Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked'))
sub.dv <- predict(dv, newdata = sub.data)
##pp <- preProcess(sub.data, method = 'knnImpute')
sub.impute <- predict(pp, sub.dv)
sub.data <- data.frame(sub.dv)
sub.data$Age <- sub.impute[,4]
sub.data$Fare <- sub.impute[,7]
imputed.data[,8:10] <- impute.data[,8:10]
preds <- predict(bagModel, sub.data)
submit <- data.frame(PassengerId = ids, Survived = preds)
write.csv(submit, 'submission.csv', row.names = FALSE)
```

#Bibliography
A lot of my basic knowledge of the R language came from the John Hopkins Data Science courses on Coursera.  
Leek, Jeff, et al. “Practical Machine Learning.” Practical Machine Learning. www.coursera.org/learn/practical-machine-learning. Accessed 1 Jan. 2018.  
  
The O'Reilly book R in a Nutshell gave me a lot of my more in-depth knowledge. It has great chapters on regression, classification, and machine learning. It can be a reference for machine learning, graphics, and base R.  
Adler, Joseph. R in a nutshell. 2nd ed., OReilly, 2012.  
  
The Caret package by Max Kuhn is the go to for higher level caret ideas and concepts.  
Kuhn, Max. “The caret Package.” The caret Package, 4 Sept. 2017, topepo.github.io/caret/.